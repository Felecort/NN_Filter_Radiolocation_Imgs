{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FGNoucCJ4rRN"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image, ImageOps\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Pytorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim  \n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import ImageFolder\n",
        "# from torch.utils.data.sampler import SubsetRandomSampler\n",
        "\n",
        "from skimage.metrics import structural_similarity as ssim\n",
        "from os import listdir\n",
        "from tqdm import tqdm\n",
        "from pathlib import Path\n",
        "\n",
        "# Additional modules\n",
        "# from dataset_creator import generate_unshuffled_csv\n",
        "from assistive_funcs import filtering_image, check_ssim, check_gmsd\n",
        "# from csv_dataloader import get_train_test_small_data\n",
        "from image_processing import add_noise\n",
        "from time import time\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "k3aBTSt76W3-"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "device(type='cuda')"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# IMAGES_DIR = Path(\"train/\")\n",
        "# TEST_IMAGES_DIR = Path(\"test/\")\n",
        "# DEVICE = torch.device(\"cuda\")\n",
        "\n",
        "\n",
        "# Paths\n",
        "p_main_data = Path(\"../data\")\n",
        "p_models = Path(\"../models\")\n",
        "\n",
        "# p_scv_folder = p_main_data / \"csv_files\"\n",
        "\n",
        "p_train_images = p_main_data / \"train\"\n",
        "p_test_images = p_main_data / \"test\"\n",
        "\n",
        "p_train_noised_images = p_main_data / \"train_noised\"\n",
        "p_test_noised_images = p_main_data / \"test_noised\"\n",
        "\n",
        "p_train_filtered_images = p_main_data / \"train_filtered\"\n",
        "p_test_filtered_images = p_main_data / \"test_filtered\"\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "F1uuYPkZ9Ozv"
      },
      "outputs": [],
      "source": [
        "RESCALE_SIZE = 608\n",
        "\n",
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, image_dir, normalize):\n",
        "        self.image_dir = image_dir\n",
        "        self.images_list = listdir(self.image_dir)\n",
        "\n",
        "        if normalize:\n",
        "            factor = 255\n",
        "        else:\n",
        "            factor = 1\n",
        "        self.x_transform = transforms.Compose([\n",
        "            transforms.Grayscale(1),\n",
        "            transforms.Resize((RESCALE_SIZE, RESCALE_SIZE)),\n",
        "            transforms.Lambda(add_noise),\n",
        "            # transforms.Normalize(0.5, 0.5),\n",
        "            transforms.Lambda(lambda x: x / factor),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        self.y_transform = transforms.Compose([\n",
        "            transforms.Grayscale(1),\n",
        "            transforms.Resize((RESCALE_SIZE, RESCALE_SIZE)),\n",
        "            transforms.Lambda(lambda x: x / factor),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.images_list)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        image = Image.open(self.image_dir / self.images_list[index])\n",
        "        noised_image = self.x_transform(image).float()\n",
        "        image = self.y_transform(image)\n",
        "\n",
        "        return noised_image, image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 106,
      "metadata": {
        "id": "CuSaFfsc_7vA"
      },
      "outputs": [],
      "source": [
        "class CNNModel_(nn.Module):\n",
        "    def __init__(self, num_filter=4):\n",
        "        super().__init__()\n",
        "\n",
        "        self.conv1 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=1, out_channels=4, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=4)\n",
        "            )\n",
        "        self.conv2 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=4, out_channels=4, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=4)\n",
        "            )\n",
        "        self.conv3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=8, out_channels=8, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=8)\n",
        "            )\n",
        "        self.conv4 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=16, out_channels=16, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=16)\n",
        "            )\n",
        "        self.conv5 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=32, out_channels=32, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=32)\n",
        "            )\n",
        "        self.conv6 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=64)\n",
        "            )\n",
        "        self.conv7 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels=128, out_channels=1, kernel_size=3, padding=\"same\"),\n",
        "            nn.ReLU(),\n",
        "            nn.BatchNorm2d(num_features=1)\n",
        "            )\n",
        "    \n",
        "    def forward(self, x):\n",
        "        out1 = self.conv1(x)\n",
        "\n",
        "        out2 = self.conv2(out1)\n",
        "        out2 = torch.cat((out1, out2), dim=1)\n",
        "\n",
        "        out3 = self.conv3(out2)\n",
        "        out3 = torch.cat((out2, out3), dim=1)\n",
        "\n",
        "        out4 = self.conv4(out3)\n",
        "        out4 = torch.cat((out3, out4), dim=1)\n",
        "\n",
        "        out5 = self.conv5(out4)\n",
        "        out5 = torch.cat((out4, out5), dim=1)\n",
        "\n",
        "        out6 = self.conv6(out5)\n",
        "        out6 = torch.cat((out5, out6), dim=1)\n",
        "\n",
        "        out7 = self.conv7(out6)\n",
        "        return out7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class FitModel():\n",
        "    def __init__(self, model, criterion, optimizer, scheduler,\n",
        "                 p_scv_folder, train_dataset_name,\n",
        "                 batch_size, device, num_epoches, normalize_data):\n",
        "        # Model\n",
        "        self.model = model\n",
        "        self.criterion = criterion\n",
        "        self.optimizer = optimizer\n",
        "        self.scheduler = scheduler\n",
        "        \n",
        "        # Params\n",
        "        self.num_epoches = num_epoches\n",
        "        self.batch_size = batch_size\n",
        "        self.device = device  \n",
        "        \n",
        "        # Folders\n",
        "        self.p_scv_folder = p_scv_folder\n",
        "        self.train_dataset_name = train_dataset_name\n",
        "        \n",
        "        # Flags\n",
        "        self.normalize_data = normalize_data\n",
        "        \n",
        "        # Constants\n",
        "        self.train_losses = []\n",
        "        self.valid_losses = []\n",
        "        self.images_filtered = False\n",
        "\n",
        "    def _train(self, current_epoch):\n",
        "        total_loss = []\n",
        "        start_time = time()\n",
        "        self.model.train()\n",
        "        for data, targets in self.train_loader:\n",
        "            \n",
        "            data = data.to(device=self.device)\n",
        "            targets = targets.to(device=self.device)\n",
        "            if self.normalize_data:\n",
        "                data /= 255\n",
        "                targets /= 255\n",
        "            \n",
        "            scores = self.model(data)\n",
        "            loss = self.criterion(scores, targets)\n",
        "            loss.backward()\n",
        "           \n",
        "            self.optimizer.step()\n",
        "            self.optimizer.zero_grad()\n",
        "            \n",
        "            total_loss.append(loss.item())\n",
        "        \n",
        "        mean_total_loss = np.mean(total_loss)\n",
        "        self.train_losses.append(mean_total_loss)\n",
        "        print(f\"Epoch: {current_epoch}/{self.num_epoches}, time: {int(time() - start_time)}s, lr = {self.scheduler.get_last_lr()}\\n\\tTrain loss: {mean_total_loss:.2f}\")\n",
        "        \n",
        "    def _valid(self, current_epoch):\n",
        "        total_loss = []\n",
        "\n",
        "        self.model.eval()\n",
        "        with torch.no_grad():\n",
        "            for data, targets in self.valid_loader:\n",
        "                \n",
        "                data = data.to(device=self.device)\n",
        "                targets = targets.to(device=self.device)\n",
        "                if self.normalize_data:\n",
        "                    data /= 255\n",
        "                    targets /= 255\n",
        "                # Forward\n",
        "                scores = self.model(data)\n",
        "                loss = self.criterion(scores, targets)\n",
        "                total_loss.append(loss.item())\n",
        "        \n",
        "        mean_total_loss = np.mean(total_loss)\n",
        "        self.valid_losses.append(mean_total_loss)\n",
        "        print(f\"\\tValid loss: {mean_total_loss:.2f}\")\n",
        "    \n",
        "    def fit(self):\n",
        "        self.train_loader, self.valid_loader = get_train_test_small_data(scv_folder=self.p_scv_folder, dataset_name=self.train_dataset_name,\n",
        "                                                        batch_size=self.batch_size, split=True)\n",
        "        for epoch in range(self.num_epoches):\n",
        "            self._train(epoch + 1)\n",
        "            self._valid(epoch + 1)\n",
        "            if self.scheduler is not None:\n",
        "                self.scheduler.step()\n",
        "                \n",
        "    def plot_graph(self):\n",
        "        sns.set()\n",
        "        fig, (ax_train, ax_test) = plt.subplots(nrows=1, ncols=2, figsize=(12, 6))\n",
        "        fig.suptitle('Loss')\n",
        "\n",
        "        ax_train.set_title(\"Train loss\")\n",
        "        ax_test.set_title(\"Valid loss\")\n",
        "\n",
        "        ax_train.set_ylabel('Loss value')\n",
        "        ax_test.set_ylabel('Loss value')\n",
        "\n",
        "        ax_train.set_xlabel(\"Batch\")\n",
        "        ax_test.set_xlabel(\"Batch\")\n",
        "\n",
        "        sns.lineplot(data=self.train_losses, ax=ax_train)\n",
        "        sns.lineplot(data=self.valid_losses, ax=ax_test)\n",
        "\n",
        "        plt.show()\n",
        "    \n",
        "    def filtering_all_images(self):\n",
        "        self.images_filtered = True\n",
        "        images_names = listdir(p_test_noised_images)\n",
        "        for name in images_names:\n",
        "            filtering_image(self.model, p_test_filtered_images, p_test_noised_images, name, win_size, self.device, normalize_data=self.normalize_data)\n",
        "        \n",
        "    @staticmethod\n",
        "    def _check_filtering(p_target_images, p_original_images):\n",
        "        ssim_metric = []\n",
        "        gmsd_metric = []\n",
        "        images_names = listdir(p_target_images)\n",
        "        for name in images_names:\n",
        "            ssim_metric.append(check_ssim(p_target_images, p_original_images, name))\n",
        "            gmsd_metric.append(check_gmsd(p_target_images, p_original_images, name))\n",
        "        return ssim_metric, gmsd_metric\n",
        "        # print(f\"SSIM avg: {sum(ssim_metric) / len(ssim_metric)}\")\n",
        "        # print(f\"GMSD avg: {sum(gmsd_metric) / len(gmsd_metric)}\")\n",
        "    \n",
        "    def check_metrics(self):\n",
        "        if not self.images_filtered:\n",
        "            print(\"Warning: images weren't filtered\")\n",
        "        metrics_after_filtering = self._check_filtering(p_test_filtered_images, p_test_images)\n",
        "        metrics_befor_filtering = self._check_filtering(p_test_noised_images, p_test_images)\n",
        "        print(f\"After filtering\\n\\tSSIM: {np.mean(metrics_after_filtering[0]):.3f}\\n\\tGMSD: {np.mean(metrics_after_filtering[1]):.3f}\")\n",
        "        \n",
        "        print(f\"Before filtering\\n\\tSSIM: {np.mean(metrics_befor_filtering[0]):.3f}\\n\\tGMSD: {np.mean(metrics_befor_filtering[1]):.3f}\")\n",
        "        \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "learning_rate = 0.005\n",
        "EPOCHES = 40\n",
        "BATCH_SIZE = 2\n",
        "NUM_FILTERS = 3\n",
        "NORMALIZE = True\n",
        "\n",
        "image_dataset = ImageDataset(IMAGES_DIR)\n",
        "image_loader = DataLoader(image_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "model = CNNModel_(num_filter=NUM_FILTERS).to(device=DEVICE)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "3.9_venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "88cdb296fa45150573a7479e53c46514a0e326ba4c5c7ce7e99a342aad6ba12b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
